# 3-3. 深層学習ライブラリ（その2）

前回主にkerasの使い方を説明しましたが、深層学習の訓練のやり方が以下のプロトコルに依存していました：
```python
model.compile(...)
model.fit(...)
```
例えば `model.compile()` でロス関数を指定していましたが、自分がさせたい訓練目標がデフォルトの設定に存在しない場合はどうすれば良いでしょうか？

そのような場合、**訓練の定義を自作**することができます。しかしそのためには、kerasの背後のライブラリのことを知らないといけないようです。Kerasの背後で動いているライブラリには以下のようなものがあります：

```
Keras
├── TensorFlow
├── PyTorch
└── JAX
```

これらのライブラリは（JAXはやや例外的ではあるものの）単独で動かすことのできる深層学習用ライブラリであり、場合のよってはどれかのライブラリでしか欲しい実装例が見つからないこともあります。そこで今回はこれらのライブラリの簡単な入門編として、
- 深層学習ワークフローの書き方
- kerasでの使用方法

を説明したいと思います。

## `TensorFlow`

TensorFlow は深層学習用のフレームワークの一つで、ユーザーの数が多く、困ったときに検索するとたくさん解決策が見つかるのが良いです。他のライブラリに比べてデプロイを意識した機能が多い印象です。インポート文は以下です。
```python
import tensorflow as tf
```

これは全世界共通です。なお、ここでは最小限の機能の説明しかしませんので、詳細は公式ドキュメント
- https://www.tensorflow.org/?hl=ja

を参照してください。

### 深層学習ワークフローの書き方

#### モデルの作り方

ややこしいのですが、歴史的な経緯があって TenforFlow の中に更に keras が入っており（`tf.keras`）、そこからモデルを作ることができます。ですがこれは実質 keras なのでモデルの作り方は前回の説明から変わりません。クラス継承する形で再度書いておくと以下のようになります：
```python
class MyModel(tf.keras.Model):
    def __init__(self):
        # 内部パラメータなどの情報
        super().__init__()
        self.層1 = tf.keras.layers.層1()
        self.層2 = tf.keras.layers.層2()
        ...
        self.層L = tf.keras.layers.層L()
        #以下は任意（model.summaryの表示に影響）
        self.call(tf.keras.Input(shape=(入力1バッチあたりのshape)))

    def call(self, inputs):
        # 実際のニューラルネット処理
        x = self.層1(inputs)
        x = self.層2(x)
        ...
        outputs = self.層L(x)
        return outputs
```


#### モデルの訓練の仕方

一方で訓練方法ですが、kerasの `model.compile()` + `model.fit()` 部分を自作することができます。

##### `model.compile()` に当たる部分

この部分の「入力」は
- `model`（上の自作モデルクラスのインスタンス）
- `optimizer` ：微分情報を使ってどのようにパラメータ更新するかを制御するオブジェクト
    > - `optimizer = tf.keras.optimizers.更新手法名()` で作る
    > - どういうのがあるかは：https://keras.io/api/optimizers/ を参照
- `loss_fn` 関数：微分をとる対象の関数
    > - `tf.keras.losses.ロス関数名()` で作成する
    > - pythonの `def` でより複雑なものも作れる
- データ配列（教師ありの設定の場合はミニバッチ `X, y` ですが、この限りではないです）

です。訓練ステップ1回は以下のように書きます：

```python
def train_step(model, optimizer, data):
    # データ読み込み
    X, y = data

    # 誤差逆伝播法のための記録処理
    with tf.GradientTape() as tape:
        y_pred = model(X, training=True) 
        loss_value = loss_fn(y, y_pred)   

    # 勾配計算
    grads = tape.gradient(loss_value, model.trainable_variables) 

    # パラメータ更新
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

前回の図に照らし合わせると以下の部分が対応します：

<img src="figs/train_tf.drawio.svg" style="width:80%">

ここでは教師ありの設定 `X, y = data` を考えましたが、モデルパラメータ $`\theta`$ に依存した関数があって、その微分更新をするというだけなので、教師ありの場合だけでなく、もっと自由に書くことができます。

##### `model.fit()` に当たる部分

あとは
- 与えられたデータをランダムに分割しミニバッチの列を作り
- ミニバッチの列方向に `for` 文で処理を書く

ということをやれば良いです。その際にコールバック的な処理をしたければ、自分で書きます。典型的には以下のようなコードになります。

```python
# 1. モデル、最適化手法、ロスなどの設定
model = MyModel()
optimizer = tf.keras.optimizers.更新手法名()
loss_fn = tf.keras以下の関数とpython構文で作った自作ロス関数
N_epochs = xxx

# 2. 訓練ループの外で、データ処理のオブジェクトとして `batches` を作る
batches = tf.data.Dataset.from_tensor_slices(data)
batches = batches.shuffle(buffer_size).batch(batch_size)

# 3. 訓練ループを回す
for epoch in range(N_epochs):
    # batches を使って各エポックは回す
    for batch in batches:
        train_step(model, optimizer, batch)
        # 1回更新ごとのコールバックが必要なら書く
    # 1エポックごとのコールバックが必要なら書く
```
`tf.data.Dataset` についての詳細は https://www.tensorflow.org/guide/data?hl=ja を見てください。


- 例：人工データ分布でtensorflowを実際に動かしてみる
    <details>
    <summary>データとモデルの用意</summary>
    <blockquote>

    前節でも使ったデータ生成器を使います。
    ```python
    class ToyDataGenerator():
        def __init__(self, dim: int, n_class: int):
            self.rg = np.random.default_rng(seed=3)
            self.mu_for_class_np = self.rg.normal(0, 1, size=(n_class, dim))
            self.dim = dim
            self.n_class = n_class
            
        def sample(self, N_batch:int):
            x = []
            y = []
            for n in range(self.n_class):
                mu = self.mu_for_class_np[n]
                x = x + (self.rg.normal(0, .2, size=(N_batch//self.n_class,self.dim)) + mu).tolist()
                y = y + (n*np.ones(shape=(N_batch//self.n_class, 1))).tolist() 
            x = np.array(x).astype(np.float32)
            y = np.array(y).astype(np.int32)
            df = pd.DataFrame(
                {"x0": x[:, 0],
                "x1": x[:, 1],
                "y": y.reshape(-1)}
                )  
            return df

    p = ToyDataGenerator(dim=2, n_class=5)
    df = p.sample(3000)
    sns.relplot(data=df, x="x0", y="x1", hue="y")
    ```
    > <img src="figs/dl_cl1.jpg" width=40%></img>
    
    モデルも前回と同じものをTensorFlowで書き換えただけにします：

    ```python
    class MyModel(tf.keras.Model):
        def __init__(self):
            # 内部パラメータなどの情報
            super().__init__()
            self.l1 = tf.keras.layers.Dense(10, activation='relu')
            self.l2 = tf.keras.layers.Dense(8, activation='relu')
            self.l3 = tf.keras.layers.Dense(5, activation='softmax')

        def call(self, inputs):
            # 実際のニューラルネット処理
            x = self.l1(inputs)
            x = self.l2(x)
            outputs = self.l3(x)
            return outputs
    ```

    </blockquote>
    </details>
    <details>
    <summary>loss_fnとtrain_stepの定義</summary>
    <blockquote>

    データが分類問題なので、ロス関数は分類のためのもの（相対エントロピー）を定義しておきます。
    なお、相対エントロピー には二種類あり
    - `SparseCategoricalCrossentropy`: ターゲットが分類ラベルを表す自然数の場合
    - `CategoricalCrossentropy`: ターゲットが分類ラベルを表す自然数の成分に1を持ち、他の成分が0のベクトルの場合

    です。ここではデータの形式が前者なので **Sparse** がついている方を選びます：
    ```python
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
    ```

    これだけでも良いのですが、以下のように自分で書いても良いです：

    ```python
    def loss_fn(y_true, y_pred):
        # 小文字の sparse_categorical_crossentropy は関数です
        loss_array = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) 
        loss_value = tf.reduce_mean(loss_array) # バッチごとに計算してくるので平均
        return loss_value
    ```

    自分で書く際は、tf の直下にある関数を使わないと学習がうまくいかないことに注意してください。numpyの関数などを使うと、そこで微分計算（誤差逆伝搬）がストップしてしまいます。
    
    train_stepですが、今回のロスは特に複雑ではないので、本文のテンプレート通りでOKです：

    ```python
    def train_step(model, optimizer, data):
        X, y = data
        with tf.GradientTape() as tape:       # 勾配計算に使う情報
            y_pred = model(X, training=True) 
            loss_value = loss_fn(y, y_pred)   
        # 勾配計算
        grads = tape.gradient(loss_value, model.trainable_variables) 
        # パラメータ更新
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        return loss_value # 後のために値を返すようにしておく
    ```
    </blockquote>
    </details>

    <details>
    <summary>訓練ループを回す</summary>
    <blockquote>
    
    あとは上のテンプレートに従って、訓練ループを書くだけです。データ周りの処理以外はほとんどコピペで済むはずです：

    ```python
    # 0. データの処理
    X_train = df[["x0", "x1"]]
    y_train = df["y"]
    data = (X_train, y_train)
    batch_size=10

    # 1. モデル、最適化手法、ロスなどの設定
    model = MyModel()
    optimizer = tf.keras.optimizers.Adam()
    N_epochs = 5

    # 2. 訓練ループの外で、データ処理のオブジェクトとして `batches` を作る
    batches = tf.data.Dataset.from_tensor_slices(data)
    batches = batches.shuffle(buffer_size=3000).batch(batch_size=batch_size)

    # 3. 訓練ループを回す
    for epoch in range(N_epochs):
        # batches を使って各エポックは回す
        for m, batch in enumerate(batches):
            loss_value = train_step(model, optimizer, batch)
            # 1回更新ごとのコールバックが必要なら書く
            if m%(len(X_train)//(10*batch_size))==0:
            print("=", end="")
        # 1エポックごとのコールバックが必要なら書く
        print(f" {epoch}-th epoch finished, loss for the final minibach={loss_value}")
    ```
    > ```
    > ========== 0-th epoch finished, loss for the final minibach=0.8673250079154968
    > ========== 1-th epoch finished, loss for the final minibach=0.2119894027709961
    > ========== 2-th epoch finished, loss for the final minibach=0.5714402794837952
    > ========== 3-th epoch finished, loss for the final minibach=0.2421935349702835
    > ========== 4-th epoch finished, loss for the final minibach=0.18907570838928223
    > ```

    なお、スピードが気になる場合は、`train_step` の定義に `@tf.function` のデコレータをつけてください。訓練後にモデルを使いたい場合は `model(X)` で良いはずです。ここでは検証は省略します。
    
    </blockquote>
    </details>

$\blacksquare$ **練習問題1:** 手書き文字認識をやってみましょう。データ読み込みは以下：
```python
from sklearn import datasets

df = datasets.load_digits()
X = df["images"]/np.max(df["images"]) # [0, 1] に収めておく
X = X.reshape(-1, 8, 8, 1)
y = df["target"]
```
<details class="memo">
<summary>ランダムにデータからサンプルを抽出して表示してみるプログラム</summary>
<blockquote>

```python
rng = np.random.default_rng(seed=1)
i = rng.integers(0, X.shape[0])

plt.figure(figsize=(2,2))
sns.heatmap(X[i], cmap="gray_r")
plt.title(f"{y[i]}")
plt.show()
```
> <img src="figs/nn_ex6.png" width=40%>

</blockquote>
</details>

loss_fn は自分で定義しても良いですが、上のデータ形式だと以下で十分です：
```python
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
```
モデルはなんでも良いです。以下を展開すると参考モデル定義が出ます：
<details>
<summary>モデルの例</summary>
<blockquote>

```python
class CNNModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.c1 = tf.keras.layers.Conv2D(4, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
        self.c2 = tf.keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
        self.c3 = tf.keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
        self.fl = tf.keras.layers.Flatten()
        self.l1 = tf.keras.layers.Dense(10, activation="softmax")
        self.call(tf.keras.Input(shape=(8,8,1)))

    def call(self, inputs, training=False):
        x = self.c1(inputs)
        x = self.c2(x)
        x = self.c3(x)
        x = self.fl(x)
        outputs = self.l1(x)
        return outputs
```
</blockquote>
</details>

> [!TIP]
> <details>
> <summary>解答例</summary>
> 
> 訓練ステップを定義しますが、本文中と全く同じで良いです。デコレータをつけておいて高速化します。
> ```python
> # @tf.function を実行するセルと同じセルで loss_fn を定義しておいた方が良いようです
> loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
> 
> @tf.function
> def train_step(model, optimizer, data):
>     # データ読み込み
>     X, y = data
> 
>     # 誤差逆伝播法のための記録処理
>     with tf.GradientTape() as tape:
>         y_pred = model(X, training=True) 
>         loss_value = loss_fn(y, y_pred)   
> 
>     # 勾配計算
>     grads = tape.gradient(loss_value, model.trainable_variables) 
> 
>     # パラメータ更新
>     optimizer.apply_gradients(zip(grads, model.trainable_variables))
>     return loss_value
> ```
> 訓練ループも同じで動くはずです。ホールドアウト検証を見越してテストデータを分けておきます。
> 
> ```python
> from sklearn import datasets
> from sklearn.model_selection import train_test_split
> 
> # 0. データの処理
> ## 読み込み
> df = datasets.load_digits()
> X = df["images"]/np.max(df["images"]) # [0, 1] に収めておく
> X = X.reshape(-1, 8, 8, 1)
> y = df["target"]
> ## 訓練データ/テストデータの分離
> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
> data = (X_train, y_train)
> batch_size=10
> 
> # 1. モデル、最適化手法、ロスなどの設定
> model = CNNModel()
> optimizer = tf.keras.optimizers.Adam()
> N_epochs = 10
> 
> # 2. 訓練ループの外で、データ処理のオブジェクトとして `batches` を作る
> batches = tf.data.Dataset.from_tensor_slices(data)
> batches = batches.shuffle(buffer_size=3000).batch(batch_size=batch_size)
> 
> # 3. 訓練ループを回す
> for epoch in range(N_epochs):
>     # batches を使って各エポックは回す
>     for m, batch in enumerate(batches):
>         loss_value = train_step(model, optimizer, batch)
>         # 1回更新ごとのコールバックが必要なら書く
>         if m%(len(X_train)//(10*batch_size))==0:
>             print("=", end="")
>     # 1エポックごとのコールバックが必要なら書く
>     print(f" {epoch}-th epoch finished, loss for the final minibach={loss_value}")
> ```
> > ```
> > =========== 0-th epoch finished, loss for the final minibach=1.8005421161651611
> > =========== 1-th epoch finished, loss for the final minibach=1.3222359418869019
> > =========== 2-th epoch finished, loss for the final minibach=0.20885062217712402
> > =========== 3-th epoch finished, loss for the final minibach=0.2899131178855896
> > =========== 4-th epoch finished, loss for the final minibach=0.34687119722366333
> > =========== 5-th epoch finished, loss for the final minibach=0.23294863104820251
> > =========== 6-th epoch finished, loss for the final minibach=0.6767892837524414
> > =========== 7-th epoch finished, loss for the final minibach=0.048489805310964584
> > =========== 8-th epoch finished, loss for the final minibach=0.1609015166759491
> > =========== 9-th epoch finished, loss for the final minibach=0.13994833827018738
> > ```
> 検証してみます：
> ```python
> rng = np.random.default_rng(seed=999)
> i = rng.integers(0, X_test.shape[0])
> 
> plt.figure(figsize=(6,2))
> 
> plt.subplot(1,2,1)
> sns.heatmap(X_test[i].reshape(8, 8), cmap="gray_r", cbar=False)
> plt.title(f"data label:{y_test[i]}")
> 
> plt.subplot(1,2,2)
> plt.title(f"model: p(x)")
> sns.barplot(model(X_test[i:i+1]))
> plt.show()
> ```
> > <img src="figs/tf_ex1.png" width=60%>
> </details>

$\blacksquare$ **練習問題2:** 分類モデルを「だます」、すなわち、モデルの出力が狙った分類ラベル $`y_\text{tricked}`$ になるように入力値に微小な変更 $`\delta \mathbf{x}`$ を加えることができます。これを **敵対的摂動 (adversarial perturbation)** などと言います。いろいろなやり方がありますが、たとえば適当に小さな $\delta \mathbf{x}$ から初めて

$$
\begin{align*}
\delta \mathbf{x} \leftarrow \delta \mathbf{x} - \epsilon \nabla_{\delta \mathbf{x}} \Big( l\big(y_\text{tricked}, f_\theta(\mathbf{x}+ \delta \mathbf{x})\big) + c |\delta \mathbf{x}| \Big)
\end{align*}
$$

（ただし $`\epsilon, c>0`$、$`l`$ は$`f_\theta`$ を訓練したロス関数）という更新式を数回繰り返すことが考えられます。TensorFlowの自動微分を $`\delta \mathbf{x}`$ に適用すれば、これも実装できるはずです。以下、バッチサイズ1として
- $`\mathbf{x}`$ = `X`, $`f_\theta`$ = `model`
- $`y_\text{tricked}`$ = `y_tricked`, $`\delta \mathbf{x}`$ = `dX`

の場合で、10回更新する例を書いたコードが「更新コード」クリックで開きます。

<details>
<summary>更新コード</summary>
<blockquote>

```python
rng = np.random.default_rng(seed=999)
i = rng.integers(0, X_test.shape[0])

X = X_test[i:i+1]
X = tf.constant(X.reshape(1, 8, 8, 1).astype(np.float32))
y_tricked = 1
y_tricked = tf.constant(np.array([y_tricked]))

optimizer = tf.keras.optimizers.SGD()
dX = tf.Variable(initial_value=X+np.random.uniform(0,.05, (1, 8, 8, 1)), trainable=True)

def step(model, optimizer, X, dX, y_tricked):
    with tf.GradientTape() as tape:
        tape.watch(dX)
        loss_value = loss_fn(model, X, dX, y_tricked)
    grads = tape.gradient(loss_value, dX)
    optimizer.apply_gradients(zip([grads], [dX]))
    return loss_value

for _ in range(10):
  print(step(model, optimizer, X, dX, y_tricked))
```

</blockquote>
</details>

このコード中、$`l\big(y_\text{tricked}, f_\theta(\mathbf{x}+ \delta \mathbf{x})\big) + c |\delta \mathbf{x}|`$ = `loss_fn(model, X, dX, y_tricked)` は未実装です。これを実装して上の更新コードを動かし、敵対的摂動を見つけてください。

- いくつかヒント
    <details>
    <summary>loss_fnに使う TensorFlow の関数 </summary>
    <blockquote>

    $l$ については、訓練の時をまねて以下の処理で良いです

    ```python
    y_pred = model(X+dX)
    loss_array = tf.keras.losses.sparse_categorical_crossentropy(y_tricked, y_pred) 
    loss_value = tf.reduce_mean(loss_array) # バッチごとに計算してくるので平均
    ```

    l1正則化部分は

    ```python
    c=0.1
    penalty = c*tf.reduce_mean(tf.abs(dX))
    ```

    でOK。

    </blockquote>
    </details>

    <details>
    <summary>更新後にモデルを騙せているかチェックするコード</summary>
    <blockquote>
    
    ```python
    plt.figure(figsize=(6,2))

    plt.subplot(1,2,1)
    sns.heatmap(X.numpy().reshape(8, 8), cmap="gray_r", cbar=False)
    plt.title(f"model's label:{np.argmax(model(X))}")

    plt.subplot(1,2,2)
    sns.heatmap((X+dX).numpy().reshape(8, 8), cmap="gray_r", cbar=False)
    plt.title(f"model's label:{np.argmax(model(X+dX))}")
    plt.show()
    ```
    </blockquote>
    </details>

> [!TIP]
> <details>
> <summary>解答例</summary>
> 
> ```python
> def loss_fn(model, X, dX, y_tricked, c=0.1):
>     y_pred = model(X+dX)
>     loss_array = tf.keras.losses.sparse_categorical_crossentropy(y_tricked, y_pred) 
>     loss_value = tf.reduce_mean(loss_array) # バッチごとに計算してくるので平均
>     penalty = c*tf.reduce_mean(tf.abs(dX))
>     return loss_value + penalty
> ```
> あとは最初のプログラムを動かすと：
> ```python
> rng = np.random.default_rng(seed=999)
> i = rng.integers(0, X_test.shape[0])
> 
> X = X_test[i:i+1]
> X = tf.constant(X.reshape(1, 8, 8, 1).astype(np.float32))
> y_tricked = 1
> y_tricked = tf.constant(np.array([y_tricked]))
> 
> optimizer = tf.keras.optimizers.SGD()
> dX = tf.Variable(initial_value=X+np.random.uniform(0,.05, (1, 8, 8, 1)), trainable=True)
> 
> def step(model, optimizer, X, dX, y_tricked):
>     with tf.GradientTape() as tape:
>         tape.watch(dX)
>         loss_value = loss_fn(model, X, dX, y_tricked)
>     grads = tape.gradient(loss_value, dX)
>     optimizer.apply_gradients(zip([grads], [dX]))
>     return loss_value
> 
> for _ in range(10):
>   print(step(model, optimizer, X, dX, y_tricked))
> ```
> > ```
> > tf.Tensor(3.675457, shape=(), dtype=float32)
> > tf.Tensor(3.1734517, shape=(), dtype=float32)
> > tf.Tensor(2.6976757, shape=(), dtype=float32)
> > tf.Tensor(2.255257, shape=(), dtype=float32)
> > tf.Tensor(1.8544785, shape=(), dtype=float32)
> > tf.Tensor(1.5072026, shape=(), dtype=float32)
> > tf.Tensor(1.2186056, shape=(), dtype=float32)
> > tf.Tensor(0.98811334, shape=(), dtype=float32)
> > tf.Tensor(0.8195424, shape=(), dtype=float32)
> > tf.Tensor(0.69427234, shape=(), dtype=float32)
> > ```
> 
> 値が減っていっているので多分うまくいっているでしょう。みてみると：
> ```python
> plt.figure(figsize=(6,2))
> 
> plt.subplot(1,2,1)
> sns.heatmap(X.numpy().reshape(8, 8), cmap="gray_r", cbar=False)
> plt.title(f"model's label:{np.argmax(model(X))}")
> 
> plt.subplot(1,2,2)
> sns.heatmap((X+dX).numpy().reshape(8, 8), cmap="gray_r", cbar=False)
> plt.title(f"model's label:{np.argmax(model(X+dX))}")
> plt.show()
> ```
> > <img src="figs/tf_ex2.png" width=60%>
> 
> となって確かに見た目は（あまり）変わっていないけれども、モデルが間違って 1 に最大確率を返していることがわかります。
> 
> </details>


### kerasでの訓練を自分で定義する

TensorFlowで訓練ループを書くと、少々長くなってしまいます。kerasの `model.fit()` を思い出すと、 
- 記述が短く簡潔
- 深層学習のワークフローがフォーマット化している
- 豊富なコールバック処理がある

などを考えると、keras がそのまま使えると便利な気がします。実は kerasのモデルクラスで `model.fit()` を実行する際には、モデルクラスの他のメソッドが色々呼び出されているだけなので、そこを上書きすれば自分なりの訓練のやり方を定義することができます。

<img src="figs/override.drawio.svg" style="width:80%">

たとえば訓練ステップ（青色部分）ではデフォルトで設定されている `train_step(self, data)` が呼び出されています。このようなことが `model.evaluate()` などのメソッドでも起こっているため、そのデフォルト設定をクラス定義の際に書き換えると、kerasのパラダイムのまま比較的自由な訓練を実行することができます。

- 詳細は公式ドキュメントの解説記事が良いです：https://keras.io/guides/custom_train_step_in_tensorflow/

#### 訓練ステップ
`train_step(self)` が呼び出されています。ですのでこのメソッドを上書きすれば、訓練ループを自分で書かずに、そのまま `model.fit()` を使えるようになります。基本は以下のフォーマットで良いようです（教師ありの場合で書きましたが、本来もっと自由に書けます）：
```python
class MyModel(keras.Model):
    ...

    def loss_fn(self, y, y_pred):
        " ロス関数の実装 "
        ...
        return loss_value

    def train_step(self, data):
        ''' 訓練ステップの実装
            すでに解説した train_step(model, optimizer, data) とほとんど同じ '''
        # データ読み込み
        X, y = data

        # 誤差逆伝播法のための記録処理
        with tf.GradientTape() as tape:
            y_pred = self(X, training=True)
            loss_value = self.loss_fn(y, y_pred)
        
        # 勾配計算
        grads = tape.gradient(loss_value, self.trainable_variables)
        # パラメータ更新 (self.optimizer に注意)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))

        return {"表示したい情報1": 値1,
                "表示したい情報2": 値2, ...} # 返り値を辞書オブジェクトにする
```

表示したい情報の部分は `model.fit()` を走らせた際に毎回のパラメータ更新時に表示される値を書きます。使用の際はたとえば以下のようにするなど考えられます：

```python
model = MyModel()
model.compile(optimizer='...')

# 訓練データ
X, y = data

# model.fit() で訓練を実行すると、上書きした train_step が呼ばれる
model.fit(X, y, epochs=..., batch_size=...)
```

#### 推論ステップ
`test_step(self)` が呼び出されています。ですのでこのメソッドを上書きすれば、そのまま `model.evaluate()` などを使えるようになります。基本は以下のフォーマットで良いようです：

```python
class MyModel(keras.Model):
    ...
    def test_step(self, data):
        " 推論用データが入ってきた時の処理を書く "
        # 何か処理をする
        return {"表示したい情報1": 値1,
                "表示したい情報2": 値2, ...} # 返り値を辞書オブジェクトにする
```

#### metricsの処理

`model.compile()` の際に引数で
- `loss="..."` や `metric=[...]` 

を指定すると、ログの処理なども自動でしてくれました。この辺りの処理がどうなっているか説明します。

##### `keras.metrics` のオブジェクト

keras内で、これらの情報を管理しているのが `keras.metrics` 以下のクラスで、以下のようなものがあります：
- `keras.metrics.Accuracy`：モデルの現在の精度を計算する
- `keras.metrics.MeanSquaredError`：モデルの現在の平均2乗ロスを計算する
- ...など、その他：https://keras.io/api/metrics/

これらのクラスから指標計算するには以下の3ステップが必要です：
1. インスタンスを作る：`self.metric = keras.metrics.指標名()`
2. 指標の値を更新する：`self.metric.update(計算に必要な情報)`
3. 指標の現在値を返す：`self.metric.result()`

これらの処理は特に `train_step(self)` や `test_step(self)` 内で有用です。具体的には、モデルクラスの初期化の段階でインスタンスを作っておき、それを後の処理で使うということです。テンプレートは以下
```python
class MyModel(keras.Model):
    def __init__(self):
        # 内部パラメータなどの情報
        ...
        # metrics
        self.metric1 = keras.metrics.指標名1()
        self.metric2 = keras.metrics.指標名2()
        ...
        self.metricN = keras.metrics.指標名N()

    @property
    def metrics(self):
        return [self.metric1, self.metric2, ..., self.metricN]
```

`@property` のデコレータをかけることで、このモデルを作った際に `model.metrics` とすると設定した指標のオブジェクトからなるリストを返してくれます。これを制定せずに、`model.compile()` で metric を指定した場合は `[<Mean name=loss>, <CompileMetrics name=compile_metrics>]` が入ります。

どの指標を使うかで若干 `.update()` に必要な引数が変わりますが、わからない場合は `help(model.metric1.update)` を読むなどしてください。


#### compileの再定義

ここまでくると `model.compile()` も再定義したくなるかもしれません。それももちろん可能です。ドキュメントをみると最低限必要なのは super関数を唱えておくことぐらいのようです。
```python
class MyModel(keras.Model):
    ...
    def compile(self, ...):
        super().compile()
        ...
```

たとえば、公式ドキュメントのGANの実装を見ると、モデル内に複数の最適化パラメータがあって、それを別々に更新したい場合は、`optimizer` を複数用意することになりますが、その場合はここで定義するようにしておくのが良さそうです。
> `optimizer` は keras.optimizers.最適化手法() でオブジェクトとして作成できます：https://keras.io/api/optimizers/

- 例：人工データ分布でkerasのカスタマイズモデルを実際に動かしてみる（metricも出す）
    <details>
    <summary>データ作成</summary>
    <blockquote>

    データはいつも通り：
    ```python
    class ToyDataGenerator():
        def __init__(self, dim: int, n_class: int):
            self.rg = np.random.default_rng(seed=3)
            self.mu_for_class_np = self.rg.normal(0, 1, size=(n_class, dim))
            self.dim = dim
            self.n_class = n_class
            
        def sample(self, N_batch:int):
            x = []
            y = []
            for n in range(self.n_class):
                mu = self.mu_for_class_np[n]
                x = x + (self.rg.normal(0, .2, size=(N_batch//self.n_class,self.dim)) + mu).tolist()
                y = y + (n*np.ones(shape=(N_batch//self.n_class, 1))).tolist() 
            x = np.array(x).astype(np.float32)
            y = np.array(y).astype(np.int32)
            df = pd.DataFrame(
                {"x0": x[:, 0],
                "x1": x[:, 1],
                "y": y.reshape(-1)}
                )  
            return df

    p = ToyDataGenerator(dim=2, n_class=5)
    df = p.sample(3000)
    sns.relplot(data=df, x="x0", y="x1", hue="y")
    ```
    > <img src="figs/dl_cl1.jpg" width=40%></img>

    </blockquote>
    </details>

    <details>
    <summary>モデル作成</summary>
    <blockquote>

    モデル定義は少し長いですが、たとえば以下のような感じ：
    ```python
    import keras

    class MyModel(keras.Model):
        def __init__(self):
            # 内部パラメータなどの情報
            super().__init__()
            self.l1 = keras.layers.Dense(10, activation='relu')
            self.l2 = keras.layers.Dense(8, activation='relu')
            self.l3 = keras.layers.Dense(5, activation='softmax')
            
            ##### 追加1（metricの表示のための処理2）#####
            self.loss_tracker = keras.metrics.Mean(name="loss")
            self.accuracy_metric = keras.metrics.SparseCategoricalAccuracy(name="accuracy")
            #########################################

        def call(self, inputs):
            # 実際のニューラルネット処理
            x = self.l1(inputs)
            x = self.l2(x)
            outputs = self.l3(x)
            return outputs

        ########## 追加2 ##########
        def train_step(self, data):
            ''' ここに1回更新あたりの処理を書く '''
            loss_value = train_step(self, self.optimizer, data) # train_step() は前の例でグローバルに定義した関数の流用

            # metricの表示のための処理2
            X, y = data
            y_pred = self(X, training=False)
            self.loss_tracker.update_state(loss_value)
            self.accuracy_metric.update_state(y, y_pred)
            return {m.name: m.result() for m in self.metrics}
        ###########################

        ########## 追加3 ##########
        @property
        def metrics(self):
            return [self.loss_tracker, self.accuracy_metric]
        ###########################
    ```
    </blockquote>
    </details>

    <details>
    <summary>訓練</summary>
    <blockquote>

    あとは compile時に optimizer を指定して fit すればOK：
    ```python
    model = MyModel()
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()

    model.compile(optimizer='adam')

    # model.fit() で訓練を実行すると、上書きした train_step が呼ばれる
    model.fit(X_train, y_train, epochs=5, batch_size=10)
    ```
    > ```
    > Epoch 1/5
    > 300/300 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.5578 - loss: 1.3826
    > Epoch 2/5
    > 300/300 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9961 - loss: 0.4211
    > Epoch 3/5
    > 300/300 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9966 - loss: 0.1422
    > Epoch 4/5
    > 300/300 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9972 - loss: 0.0634
    > Epoch 5/5
    > 300/300 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9975 - loss: 0.0340
    > <keras.src.callbacks.history.History at 0x781eb5c89c90>
    > ```
    
    </blockquote>
    </details>

$\blacksquare$ **練習問題3:** 手書き文字認識をkeras+tfで書いてみましょう。基本は以下を展開したものを使ってください：

- テンプレート：
    <details>
    <summary>データ</summary>
    <blockquote>
    
    ```python
    from sklearn import datasets

    df = datasets.load_digits()
    X = df["images"]/np.max(df["images"]) # [0, 1] に収めておく
    X = X.reshape(-1, 8, 8, 1)
    y = df["target"]
    ```
    </blockquote>
    </details>
    <details>
    <summary>モデルクラス</summary>
    <blockquote>
    
    ```python
    class CNNModel(keras.Model):
        def __init__(self):
            super().__init__()
            self.c1 = keras.layers.Conv2D(4, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
            self.c2 = keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
            self.c3 = keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
            self.fl = keras.layers.Flatten()
            self.l1 = keras.layers.Dense(10, activation="softmax")
            self.call(keras.Input(shape=(8,8,1)))

        def call(self, inputs, training=False):
            x = self.c1(inputs)
            x = self.c2(x)
            x = self.c3(x)
            x = self.fl(x)
            outputs = self.l1(x)
            return outputs
    ```
    </blockquote>
    </details>
    <details>
    <summary>metrics</summary>
    <blockquote>
    
    ```python
    loss_tracker = keras.metrics.Mean(name="My_loss")
    accuracy_metric = keras.metrics.SparseCategoricalAccuracy(name="My_accuracy")
    ```
    それぞれアップデート法は
    ```python
    loss_tracker.update(loss_value)
    accuracy_metric.update(y, y_pred)
    ```
    です。
    </blockquote>
    </details>
    <details>
    <summary>train_step関数</summary>
    <blockquote>
    
    ```python
    def train_step(model, optimizer, loss_fn, data):
        # データ読み込み
        X, y = data
        # 誤差逆伝播法のための記録処理
        with tf.GradientTape() as tape:
            y_pred = model(X, training=True) 
            loss_value = loss_fn(y, y_pred)   
        # 勾配計算
        grads = tape.gradient(loss_value, model.trainable_variables) 
        # パラメータ更新
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        return loss_value
    ```
    </blockquote>
    </details>

これらをうまく組み合わせて、モデルクラスを定義し直すことで、以下のような訓練結果が出るようにしてください：

```python
model = CNNModel()
model.compile(optimizer="adam") # loss や metric はここには書かない

model.fit(X, y, epochs=5, batch_size=10)
```
> ```
> Epoch 1/5
> 180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 4ms/step - My_accuracy: 0.2061 - My_loss: 2.1905
> Epoch 2/5
> 180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - My_accuracy: 0.7347 - My_loss: 1.0308
> Epoch 3/5
> 180/180 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - My_accuracy: 0.8355 - My_loss: 0.6477
> Epoch 4/5
> 180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - My_accuracy: 0.8587 - My_loss: 0.4827
> Epoch 5/5
> 180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - My_accuracy: 0.8801 - My_loss: 0.3889
> <keras.src.callbacks.history.History at 0x7eb52d3014d0>
> ```

> [!TIP]
> <details>
> <summary>解答例</summary>
> 
> train_stepを外部で定義する必要はないです：
> ```python
> 
> class CNNModel(keras.Model):
>     def __init__(self):
>         super().__init__()
>         self.c1 = keras.layers.Conv2D(4, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
>         self.c2 = keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
>         self.c3 = keras.layers.Conv2D(8, kernel_size=3, strides=(1,1), padding="valid", activation="relu")
>         self.fl = keras.layers.Flatten()
>         self.l1 = keras.layers.Dense(10, activation="softmax")
>         self.call(keras.Input(shape=(8,8,1)))
>         ###
>         self.loss_fn = keras.losses.SparseCategoricalCrossentropy()
>         self.loss_tracker = keras.metrics.Mean(name="My_loss")
>         self.accuracy_metric = keras.metrics.SparseCategoricalAccuracy(name="My_accuracy")
> 
>     @property
>     def metrics(self):
>         return [self.loss_tracker, self.accuracy_metric]
> 
>     def call(self, inputs, training=False):
>         x = self.c1(inputs)
>         x = self.c2(x)
>         x = self.c3(x)
>         x = self.fl(x)
>         outputs = self.l1(x)
>         return outputs
> 
>     def train_step(self, data):
>         loss_value = train_step(self, self.optimizer, self.loss_fn, data)
> 
>         X, y = data
>         y_pred = self(X, training=False)
>         self.loss_tracker.update_state(loss_value)
>         self.accuracy_metric.update_state(y, y_pred)
>         return {m.name: m.result() for m in self.metrics}
> ```
> </details>

$\blacksquare$ **練習問題4:** 上の解答例だと
```python
model.evaluate(X, y, return_dict=True)
```
はエラーが出て動きません。これを改善して
> ```
> 57/57 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - My_accuracy: 0.9296 - My_loss: 0.2621
> {'My_accuracy': 0.9293266534805298, 'My_loss': 0.270922988653183}
> ```
のような表示が出るようにしてください。
> [!TIP]
> <details>
> <summary>解答例</summary>
> 
> 元のモデルを継承して、test_stepメソッドを追加すると良いです：
> ```python
> class CNNModel2(CNNModel):
>     def test_step(self, data):
>         X, y = data
>         y_pred = self(X, training=False)
> 
>         loss_value = self.loss_fn(y, y_pred)
>         self.loss_tracker.update_state(loss_value)
>         self.accuracy_metric.update_state(y, y_pred)
>         return {m.name: m.result() for m in self.metrics}
> ```
> このクラスからモデルを作ると良いです。
> </details>


## `PyTorch`

PyTorch は深層学習用のフレームワークの一つで、ユーザーの数が多く、困ったときに検索するとたくさん解決策が見つかるのが良いです。他のライブラリに比べて細かい設定をユーザーが手で書くような仕様であり、研究者が使うことが多い印象です。インポート文は以下です。

```python
import torch
```

### 深層学習ワークフローの書き方

#### モデルの作り方

こちらにも色々作り方があるようですが、モデルクラスを継承するのが（他のライブラリも同じような書き方ができるので）覚えることが少ない気がします。

```python
class MyModel(torch.nn.Module):
    def __init__(self):
        # 内部パラメータなどの情報
        super().__init__()
        self.層1 = torch.nn.層1()
        self.層2 = torch.nn.層2()
        ...
        self.層L = torch.nn.層L()

    def forward(self, inputs):
        ''' call ではない？'''
        # 実際のニューラルネット処理
        x = self.層1(inputs)
        x = self.層2(x)
        ...
        outputs = self.層L(x)
        return outputs
```

#### モデルの訓練の仕方

訓練方法ですが、kerasの `model.compile()` + `model.fit()` 部分を自作するのがデフォルトです。

##### `model.compile()` に当たる部分

この部分の「入力」は
- `model`（上の自作モデルクラスのインスタンス）
- `optimizer` ：微分情報を使ってどのようにパラメータ更新するかを制御するオブジェクト
    > ```python
    > optimizer = torch.optim.更新手法名(model.parameters())
    > ```
    > で作ります。どういうのがあるかは：https://pytorch.org/docs/stable/optim.html を参照。
- `loss_fn` 関数：微分をとる対象の関数
    > ```python
    > loss_fn = torch.nn.ロス関数名()
    > ```
    > で作ることができますが、これだけだと、さほどできることに自由度はないです。実は pyrhonの関数定義を使って
    > ```python
    > def loss_fn(y, y_pred):
    >     # torch の関数と python の処理を組み合わせて書く
    >     return loss_value
    > ```
    > とするとより複雑なロス関数を実装することができます。
- データ配列（教師ありの設定の場合はミニバッチ `X, y` ですが、この限りではないです）

です。`loss_fn` 関数は毎回の入力にしなくても、最初に定義したものを使い続けるとかでも良いです。その場合の教師ありの場合の疑似コードは以下

```python
loss_fn = torch以下の関数とpython構文で作った自作ロス関数

def train_step(model, optimizer, data):
    X, y = data

    optimizer.zero_grad()               # 勾配情報の消去
    y_pred = model(X, training=True)
    loss_value = loss_fn(y, y_pred)

    # 勾配計算
    loss_value.backward()
    # パラメータ更新
    optimizer.step()
```

前回の図に照らし合わせると以下の部分が対応します：

<img src="figs/train_torch.drawio.svg" style="width:60%">

TenforFlow と異なる点は、PyTorchでは
1. 順伝播が常に記録モードになっており、いつでも誤差逆伝搬ができる（計算グラフが生成される）
2. `optimizer` は `model` のパラメータと同期している
3. `optimizer` に 勾配情報が加算された状態で蓄えられてゆく

という仕様になっています。1,2のおかげで記述が楽になりますが、3は少し面倒かもしれませんが、並列化やRNNの訓練の際に便利なようです。普通に使う際には、パラメータ更新前か後に、現在の貯められている勾配の値をゼロにする処理（`optimizer.zero_grad()`）をする必要があります。

##### `model.fit()` に当たる部分

あとはやはり TensorFlowと同じです：
```python
model = MyModel()
optimizer = torch.optim.更新手法名(model.parameters())
loss_fn = torch以下の関数とpython構文で作った自作ロス関数
N_epochs = xxx

for epoch in range(N_epochs):
    batches = データを適当に分ける処理(data)
    for batch in batches:
        train_step(model, optimizer, batch) # loss_fn はグローバルに定義されているものが内部で使用される
        1回更新ごとのコールバックが必要なら書く
    1エポックごとのコールバックが必要なら書く
```


### kerasでの使用方法

https://keras.io/guides/custom_train_step_in_torch/

$\blacksquare$ **練習問題5:** 手書き文字認識をtorchで書く
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

$\blacksquare$ **練習問題6:** keras+torchで同じことをやる？ 
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

## `JAX`

TensorFlow/PyTorch では配列を（GPU使用も見据えて）処理するために numpy 配列ではなく独自の配列オブジェクトを使っていましたが、そこの機能と深層学習で使う処理を分けた方が良いという考え方もあります。JAXはいわば
- GPU対応したnumpy に数値自動微分機能をつけたようなライブラリ

で、インポート文は以下です。

```python
import jax
```

> [!NOTE]
> JAX は 関数型言語の思想を強めに反映したライブラリで、なるべくいろんな処理を「純粋な数学の意味での関数」として実装しているようです。例えば
> - 機械学習モデルのパラメータ $`\theta`$ は、モデルオブジェクトに格納しないで、その値のまま取り扱う
> - 擬似乱数は内部状態を明示的に更新する処理を書いて、どんなシード値を固定/変化させているかを明確にする
> - 高速化のための jit(just in time) 処理が `@jax.jit` で可能ですが、これで関数をデコレートすると返り値以外の処理（print文など）は無視される
>
> などが挙げられます。特に擬似乱数周辺はシード値を固定することに慣れていない場合はバグの温床になりがちなので注意してください。
> ただし、keras や flax から使う場合は純粋な関数型の思想からは外れ、普通のオブジェクト指向っぽい使い方をすることになります。

### 深層学習ワークフローの書き方

#### モデルの作り方

JAX自体にニューラルネットのモデルを作ったり、adamなどの勾配降下法の発展版を使う機能はありません（作ることは可能です）。ニューラルネットを作る場合には Flax（2025年現在でも開発が活発） というライブラリを使うと良いかと思います。

```python
import flax
```

Flax でのニューラルネット周りの処理は、
- `flax.linen`（JAXの関数型の思想をある程度反映している、2024年までのデフォルト）
- `flax.nnx`（ほとんどオブジェクト指向、2024年からのデフォルト） 

の二種類があります。以下では後者でのモデル作成のテンプレートを示します：

```python
from flax import nnx

class MyModel(nnx.Module):
    def __init__(self, rngs: nnx.Rngs): # pythonでは関数の入力の型指定ができ、ここでは rngs が nnx.Rngs クラスのオブジェクトであることを意味します
        # 内部パラメータなどの情報
        self.層1 = nnx.層1(..., rngs=rngs)
        self.層2 = nnx.層2(..., rngs=rngs)
        ...
        self.層L = nnx.層L(..., rngs=rngs)

    def __call__(self, inputs):
        # 実際のニューラルネット処理
        x = self.層1(inputs)
        x = self.層2(x)
        ...
        outputs = self.層L(x)
        return outputs

model = MyModel(rngs=nnx.Rngs(整数)) # 作成の際に初期化用のシードを指定する使用らしいです
```

#### モデルの訓練の仕方

訓練方法ですが、kerasの `model.compile()` + `model.fit()` 部分を自作するのがデフォルトです。

##### `model.compile()` に当たる部分

この部分の「入力」は
- `model`（上の自作モデルクラスのインスタンス）
- `optimizer` ：微分情報を使ってどのようにパラメータ更新するかを制御するオブジェクト
    > ```python
    > import optax
    > optimizer = nnx.Optimizer(model, optax.更新手法名())
    > ```
    > で作ります。`optax` というのは JAX, Flax とも違う別のライブラリですが、 JAXベースでの パラメータの更新手法が色々入っているライブラリです。詳しくは https://optax.readthedocs.io/en/latest/ を参照。
- `loss_fn` 関数：微分をとる対象の関数
    > これまでのやり方と違い、ロス関数の第一引数はモデルオブジェクト自身とするようです：
    > ```python
    > def loss_fn(model, data): # data は後述の train_step() 関数での指定でもOK
    >     # jax/optax の関数と python の処理を組み合わせて書く
    >     return loss_value
    > ```
- データ配列（教師ありの設定の場合はミニバッチ `X, y` ですが、この限りではないです）

です。`loss_fn` 関数は毎回の入力にしなくても、最初に定義したものを使い続けるとかでも良いです。その場合の教師ありの場合の疑似コードは以下

```python
loss_fn = 自作ロス関数で第一引数が model のもの

def train_step(model, optimizer, data):
    # 勾配計算
    grads = nnx.grad(loss_fn)(model, data)
    # パラメータ更新
    optimizer.update(grads)
```

前回の図に照らし合わせると以下の部分が対応します：

<img src="figs/train_flax.drawio.svg" style="width:60%">

TenforFlow/PyTorch と異なる点は、Flax(JAX) では
- ロス関数の導関数を作って、そこに値を流し込む

という仕様になっています。また、この辺りの処理に `@nnx.jit` のデコレータをつけておくと特にGPUやTPU上で動かす際にチップ専用のコンパイルをして高速化してくれるようです。


##### `model.fit()` に当たる部分

あとはやはり これまでと同じです：
```python
model = MyModel()
optimizer = nnx.Optimizer(model, optax.更新手法名())
loss_fn = 自作ロス関数で第一引数が model のもの
N_epochs = xxx

for epoch in range(N_epochs):
    batches = データを適当に分ける処理(data)
    for batch in batches:
        train_step(model, optimizer, batch) # loss_fn はグローバルに定義されているものが内部で使用される
        1回更新ごとのコールバックが必要なら書く
    1エポックごとのコールバックが必要なら書く
```

### kerasでの使用方法

Flaxは使わない模様です。

https://keras.io/guides/custom_train_step_in_jax/

$\blacksquare$ **練習問題7:** 手書き文字認識をflaxで書く
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

$\blacksquare$ **練習問題8:** keras+jax
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

## GPUの使用

### 1枚だけ使用

#### Keras

### 複数枚使用

$\blacksquare$ **練習問題9:** スピード比較？ 
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

## おまけ：HaggingFaceのライブラリ

$\blacksquare$ **練習問題10:** 何かモデルを落として使う 
> [!TIP]
> <details open>
> <summary>解答例</summary>
> 
> </details>

[TODO] ここに入れるなら 3-2.md での 4-1.md コメントを修正

[前のsection](3-1.md) | [次のsection](../section4/4-1.md)